{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27bffd3-d835-4872-a8d3-fbeeaf3307b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00cac3e-0815-4e4d-a462-ff3b18201ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sys\n",
    "from operator import add\n",
    "from typing import Iterable, Tuple\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "#from helper import parseNeighborURLs, wrap_neighbor, combine_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde964d3-70a8-4e26-903f-e9b89be8a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CLOUDPICKLE_DISABLE_FASTPATH\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef7dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseNeighborURLs(urls: str) -> Tuple[str, str]:\n",
    "    parts = re.split(r'\\s+', urls.strip())\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Helper function to calculate rank contributions from neighbors\n",
    "def calculateRankContrib(urls, rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def initialize_rank(url_neighbors: Tuple[str, Iterable[Tuple[str, str]]]) -> Tuple[str, float]:\n",
    "    url, _ = url_neighbors\n",
    "    return (url, 1.0)\n",
    "\n",
    "# Named function to compute contributions for join result\n",
    "def computeContrib(url_urls_rank: Tuple[str, Tuple[Iterable[str], float]]) -> Iterable[Tuple[str, float]]:\n",
    "    # url_urls_rank[1] is a tuple: (neighbors, rank)\n",
    "    neighbors, rank = url_urls_rank[1]\n",
    "    return list(calculateRankContrib(neighbors, rank))\n",
    "\n",
    "def wrap_neighbor(neighbor):\n",
    "    return [neighbor]\n",
    "\n",
    "def combine_neighbors(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7953ddb-8f79-44db-841d-7ac4d3835ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/10 04:09:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+--------------------+---+--------------------+---+---------------+-----+------+------+-------+----+-------------+---------+----+\n",
      "|_c0| _c1|_c2|_c3|                 _c4|_c5|                 _c6|_c7|            _c8|  _c9|  _c10|  _c11|   _c12|_c13|         _c14|timestamp|cca2|\n",
      "+---+----+---+---+--------------------+---+--------------------+---+---------------+-----+------+------+-------+----+-------------+---------+----+\n",
      "|  5|1217| AE|ARE|United Arab Emirates|501|device-mac-501e4O...| 48|  213.42.16.154|   24|yellow|    54|Celsius|  16|1458444054343|     null|  AE|\n",
      "|  1|1189| AR|ARG|           Argentina|319|meter-gauge-319Y3...| 54| 200.71.236.145|-34.6|yellow|-58.38|Celsius|  25|1458444054287|     null|  AR|\n",
      "|  0| 915| AR|ARG|           Argentina|227|meter-gauge-2273p...| 34|  200.71.230.81|-34.6| green|-58.38|Celsius|  15|1458444054251|     null|  AR|\n",
      "|  8|1386| AR|ARG|           Argentina|763|meter-gauge-763JW...| 82|    200.55.0.70|-34.6|yellow|-58.38|Celsius|  21|1458444054404|     null|  AR|\n",
      "|  0| 861| AR|ARG|           Argentina|943|meter-gauge-943BT...| 77|  200.59.128.19|-34.6| green|-58.38|Celsius|  33|1458444054435|     null|  AR|\n",
      "|  6|1328| AT|AUT|             Austria| 75|device-mac-75OLmC...| 96| 143.161.246.65| 48.2|yellow| 16.37|Celsius|  12|1458444054168|     null|  AT|\n",
      "|  2|1522| AT|AUT|             Austria|257|meter-gauge-257AT...| 26|   87.243.133.1| 47.2|   red| 14.83|Celsius|  16|1458444054266|     null|  AT|\n",
      "|  5| 939| AT|AUT|             Austria| 21|  device-mac-21sjz5h| 44|193.200.142.254| 48.2| green| 16.37|Celsius|  30|1458444054131|     null|  AT|\n",
      "|  6| 917| AT|AUT|             Austria|369|device-mac-369rYH...| 25|  193.239.188.1| 48.2| green| 16.37|Celsius|  23|1458444054303|     null|  AT|\n",
      "|  2| 816| AT|AUT|             Austria|504|sensor-pad-504Kdi...| 78| 87.243.151.193|47.27| green|  11.4|Celsius|  32|1458444054344|     null|  AT|\n",
      "|  4|1042| AT|AUT|             Austria|758| sensor-pad-7589QBtr| 48|   62.218.4.130| 48.2|yellow| 16.37|Celsius|  30|1458444054403|     null|  AT|\n",
      "|  0| 941| AT|AUT|             Austria|974|sensor-pad-974x9dkX1| 53| 84.116.216.166| 48.2| green| 16.37|Celsius|  26|1458444054439|     null|  AT|\n",
      "|  8|1287| AT|AUT|             Austria|236|sensor-pad-2369xz...| 47|  217.25.119.17| 48.2|yellow| 16.37|Celsius|  22|1458444054256|     null|  AT|\n",
      "|  7| 904| AT|AUT|             Austria|294|sensor-pad-294FMZ...| 26|     83.65.45.1| 48.2| green| 16.37|Celsius|  14|1458444054279|     null|  AT|\n",
      "|  3| 826| AT|AUT|             Austria|483|device-mac-483Tyi...| 90| 84.116.245.201| 48.2| green| 16.37|Celsius|  16|1458444054339|     null|  AT|\n",
      "|  1|1196| AT|AUT|             Austria|585|device-mac-5851AntHC| 51|   84.116.252.9| 48.2|yellow| 16.37|Celsius|  27|1458444054364|     null|  AT|\n",
      "|  5|1543| AT|AUT|             Austria|767|meter-gauge-767rd...| 65|  195.222.121.1| 48.2|   red| 16.37|Celsius|  19|1458444054405|     null|  AT|\n",
      "|  8| 895| AT|AUT|             Austria|977|meter-gauge-977yB...| 52|     83.65.95.1| 48.2| green| 16.37|Celsius|  11|1458444054440|     null|  AT|\n",
      "|  1| 811| AT|AUT|             Austria|271|meter-gauge-271BjIL0| 31|  149.148.140.1| 48.2| green| 16.37|Celsius|  16|1458444054271|     null|  AT|\n",
      "|  1|1227| AU|AUS|           Australia|239|meter-gauge-239qJpL8| 88| 198.142.57.219|  -27|yellow|   133|Celsius|  19|1458444054258|     null|  AU|\n",
      "+---+----+---+---+--------------------+---+--------------------+---+---------------+-----+------+------+-------+----+-------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "\t.appName(\"DS5110: my awesome Spark program\") \\\n",
    "\t.master(\"spark://172.31.79.76:7077\") \\\n",
    "\t.config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# You can read the data from a file into DataFrames\n",
    "df = spark.read.csv(\"hdfs://172.31.79.76:9000/export.csv\")\n",
    "\n",
    "df = df.withColumn(\"timestamp\", col(\"_c14\").cast(\"int\"))\n",
    "df = df.withColumn(\"cca2\", col(\"_c2\").cast(StringType()))\n",
    "\n",
    "sorted_df = df.orderBy([\"cca2\", \"timestamp\"])\n",
    "\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a70098-1fed-4adb-944f-4cef6c20b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sorted_df.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.79.76:9000/ubuntu/part2_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac260cb4-c07b-4ff4-adde-401f9b7f9ea4",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0291c9-ca57-4ed8-b91f-330075fbdfa7",
   "metadata": {},
   "source": [
    "## App 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51639de0-0dab-41bd-a4c1-df7e621e178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/10 04:09:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:================================================>       (12 + 2) / 14]\r"
     ]
    }
   ],
   "source": [
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"PageRank\") \\\n",
    "        .master(\"spark://172.31.79.76:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"2048M\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Read the input file into RDD\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.79.76:9000/web-BerkStan.txt\")\n",
    "\n",
    "#numPartitions = 100\n",
    "\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)) \\\n",
    "                    .distinct() \\\n",
    "                    .groupByKey()\n",
    "\n",
    "# Initialize Ranks RDD (Each URL starts with a rank of 1.0)\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "# Iteratively calculate new ranks (for 10 iterations)\n",
    "for iteration in range(10):\n",
    "    contribsRDD = linksRDD.join(ranksRDD).flatMap(\n",
    "        lambda url_urls_rank: calculateRankContrib(url_urls_rank[1][0], url_urls_rank[1][1])\n",
    "    )\n",
    "\n",
    "    # Compute new ranks\n",
    "    ranksRDD = contribsRDD.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
    "    \n",
    "# Sort the ranks in descending order\n",
    "sortedRanksRDD = ranksRDD.sortBy(lambda x: -x[1])\n",
    "\n",
    "# Output the top 50 results\n",
    "top_50 = sortedRanksRDD.take(50)\n",
    "for rank in top_50:\n",
    "    print(rank)\n",
    "    \n",
    "# Save the results to HDFS as CSV\n",
    "output_file_path = \"hdfs://172.31.79.76:9000/output_ranks_g\"\n",
    "sortedRanksRDD.saveAsTextFile(output_file_path)\n",
    "\n",
    "ranksDF = spark.createDataFrame(sortedRanksRDD, [\"URL\", \"Rank\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92a019-23cb-4123-946a-65a8d6840d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to HDFS as CSV\n",
    "ranksDF.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.79.76:9000/part2_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9af57f-60c5-4aab-b6c5-f250d9849dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to HDFS\n",
    "ranksDf = ranksRDD.toDF()\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"hdfs://172.31.79.76:9000/output_ranks\")\n",
    "ranksDf.write.format(\"csv\").save(\"hdfs://172.31.79.76:9000/output_ranks_file2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea8ada",
   "metadata": {},
   "source": [
    "## App 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff2be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom number of partitions\n",
    "numPartitions = 100\n",
    "\n",
    "# Read the input file into RDD and repartition\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.79.76:9000/web-BerkStan.txt\") \\\n",
    "                             .repartition(numPartitions)\n",
    "\n",
    "# Group links by key using custom partitioning\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)) \\\n",
    "                    .distinct() \\\n",
    "                    .groupByKey(numPartitions)\n",
    "\n",
    "# Initialize Ranks RDD (Each URL starts with a rank of 1.0)\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "# Iteratively calculate new ranks (for 10 iterations)\n",
    "for iteration in range(10):\n",
    "    contribsRDD = linksRDD.join(ranksRDD).flatMap(\n",
    "        lambda url_urls_rank: calculateRankContrib(url_urls_rank[1][0], url_urls_rank[1][1])\n",
    "    )\n",
    "\n",
    "    # Compute new ranks with explicit number of partitions\n",
    "    ranksRDD = contribsRDD.reduceByKey(lambda x, y: x + y, numPartitions) \\\n",
    "                          .mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
    "\n",
    "# Sort the ranks in descending order using custom partitioning\n",
    "sortedRanksRDD = ranksRDD.sortBy(lambda x: -x[1], ascending=False, numPartitions=numPartitions)\n",
    "\n",
    "# Output the top 50 results\n",
    "top_50 = sortedRanksRDD.take(50)\n",
    "for rank in top_50:\n",
    "    print(rank)\n",
    "\n",
    "# Save the results to HDFS\n",
    "output_file_path = \"hdfs://172.31.79.76:9000/output_ranks_custom\"\n",
    "sortedRanksRDD.saveAsTextFile(output_file_path)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "ranksDF = spark.createDataFrame(sortedRanksRDD, [\"URL\", \"Rank\"])\n",
    "ranksDF.write.csv(\"hdfs://172.31.79.76:9000/output_ranks_custom.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ff227",
   "metadata": {},
   "source": [
    "## App 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom number of partitions\n",
    "numPartitions = 100\n",
    "\n",
    "# Increase the number of allowed task failures to simulate recovery\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"PageRank\") \\\n",
    "        .master(\"spark://172.31.79.76:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"2048M\") \\\n",
    "        .config(\"spark.task.maxFailures\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Read the input file into RDD and repartition\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://172.31.79.76:9000/web-BerkStan.txt\") \\\n",
    "                             .repartition(numPartitions)\n",
    "\n",
    "# Group links by key using custom partitioning\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)) \\\n",
    "                    .distinct() \\\n",
    "                    .groupByKey(numPartitions)\n",
    "\n",
    "# Initialize Ranks RDD (Each URL starts with a rank of 1.0)\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "# Iteratively calculate new ranks (for 10 iterations)\n",
    "for iteration in range(10):\n",
    "    contribsRDD = linksRDD.join(ranksRDD).flatMap(\n",
    "        lambda url_urls_rank: calculateRankContrib(url_urls_rank[1][0], url_urls_rank[1][1])\n",
    "    )\n",
    "\n",
    "    # Introduce a delay mid-way to simulate failure opportunity\n",
    "    if iteration == 5:\n",
    "        import time\n",
    "        print(\"Pausing to allow worker failure...\")\n",
    "        time.sleep(10)  # Give time to kill a worker manually\n",
    "\n",
    "    # Compute new ranks with explicit number of partitions\n",
    "    ranksRDD = contribsRDD.reduceByKey(lambda x, y: x + y, numPartitions) \\\n",
    "                          .mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
    "\n",
    "# Sort the ranks in descending order using custom partitioning\n",
    "sortedRanksRDD = ranksRDD.sortBy(lambda x: -x[1], ascending=False, numPartitions=numPartitions)\n",
    "\n",
    "# Output the top 50 results\n",
    "top_50 = sortedRanksRDD.take(50)\n",
    "for rank in top_50:\n",
    "    print(rank)\n",
    "\n",
    "# Save the results to HDFS\n",
    "output_file_path = \"hdfs://172.31.79.76:9000/output_ranks_failure\"\n",
    "sortedRanksRDD.saveAsTextFile(output_file_path)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "ranksDF = spark.createDataFrame(sortedRanksRDD, [\"URL\", \"Rank\"])\n",
    "ranksDF.write.csv(\"hdfs://172.31.79.76:9000/output_ranks_failure.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d589aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04832750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
