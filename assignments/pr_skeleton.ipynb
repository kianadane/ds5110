{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27bffd3-d835-4872-a8d3-fbeeaf3307b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.3.1-bin-hadoop3')\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a49878-2895-425a-9aa4-76ff2a3212a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a skeleton of the PageRank algorithm.\n",
    "Feel free to use any piece of code in this provided skeleton source file.\n",
    "To use it, you will need to copy it into your Notebook. \n",
    "Feel free to make modifications to template code as you see fit.\n",
    "However, you are encouraged to implement the algorithm completely on\n",
    "your own. :-)\n",
    "\"\"\"\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde73f3-476a-4174-b08b-9a974fd4280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper function to calculates URL contributions to the rank of other URLs\"\"\"\n",
    "def calculateRankContrib(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "\"\"\"Helper function to parses a urls string into urls pair\"\"\"\n",
    "def parseNeighborURLs(urls: str) -> Tuple[str, str]:\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eba1ea-2621-42a3-bb42-af7167bd3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Spark context\n",
    "# TODO: You should define a new name for your PySpark PageRank program\n",
    "spark = (SparkSession.builder.appName(\"A2:PageRank\")\n",
    "    .master(\"spark://<IP_addr_vm1>:7077\")\n",
    "    .config(\"spark.executor.memory\", \"2048M\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba557ab-2154-4e0e-b7e4-f416b53ed712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hint: You want to use hash partitioning to optimize the performance of join\"\"\"\n",
    "\n",
    "# Loads in input file\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "linesRDD = spark.sparkContext.textFile(\"hdfs://<IP_addr_vm1>:9000/<input_file>\")\n",
    "\n",
    "# Perform a transformation to define a links RDD by using parseNeighborURLs helper function\n",
    "linksRDD = linesRDD.map(lambda urls: parseNeighborURLs(urls)).distinct().groupByKey()\n",
    "\n",
    "# Initialize a ranks RDD\n",
    "ranksRDD = linksRDD.map(lambda url_neighbors: (url_neighbors[0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43731680-d287-4c3c-b849-7cd587af3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "# Replace N with a number ranging from 3 to 10\n",
    "# For A2 you are required to complete 10 iterations\n",
    "for iteration in range(N)):\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here\n",
    "    # TODO: Implement the PageRank algorithm here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9af57f-60c5-4aab-b6c5-f250d9849dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to HDFS\n",
    "ranksDf = ranksRDD.toDF()\n",
    "# Note: You should not overwrite the input file. Otherwise you need to reload it\n",
    "ranksDf.write.format(\"csv\").save(\"hdfs://<IP_addr_vm1>:9000/<output_file>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79490e22-043d-40da-9f37-8d89070a6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionally) stop the Spark session when it's done\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
